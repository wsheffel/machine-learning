{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide-cell"
        ],
        "id": "0C3oJsawmIEc"
      },
      "outputs": [],
      "source": [
        "# Install the necessary dependencies\n",
        "\n",
        "import os\n",
        "import sys\n",
        "!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "remove-cell"
        ],
        "id": "puosjc-WmIEd"
      },
      "source": [
        "---\n",
        "license:\n",
        "    code: MIT\n",
        "    content: CC-BY-4.0\n",
        "github: https://github.com/ocademy-ai/machine-learning\n",
        "venue: By Ocademy\n",
        "open_access: true\n",
        "bibliography:\n",
        "  - https://raw.githubusercontent.com/ocademy-ai/machine-learning/main/open-machine-learning-jupyter-book/references.bib\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9XDwuLAmIEf"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Preprocessing in NLP is a means to get text data ready for further processing or analysis. Most of the time, preprocessing is a mix of cleaning and normalising techniques that make the text easier to use for the task at hand.\n",
        "\n",
        "A useful library for processing text in Python is the Natural Language Toolkit (NLTK). This chapter will go into 6 of the most commonly used pre-processing steps and provide code examples so you can start using the techniques immediately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csBNb7D7mIEf"
      },
      "source": [
        "## Common NLTK preprocessing steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmHUIjOzmIEg"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Splitting the text into individual words or subwords (tokens).\n",
        "\n",
        ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/tokenization.png\n",
        "---\n",
        "name: 'tokenization in nlp'\n",
        "width: 90%\n",
        "---\n",
        "Tokenization in NLP\n",
        ":::\n",
        "\n",
        "Here is how to implement tokenization in NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oZwHo9lBmIEg",
        "outputId": "f0e7ea83-53d3-4e87-bf43-406ade671456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fdc6cdecf372>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokens:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "print(\"Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTPKC_xfmIEh"
      },
      "source": [
        "### Remove stop words\n",
        "\n",
        "Removing common words that do not add significant meaning to the text, such as ‚Äúa,‚Äù ‚Äúan,‚Äù and ‚Äúthe.‚Äù\n",
        "\n",
        "To remove common stop words from a list of tokens using NLTK, you can use the **nltk.corpus.stopwords.words()** function to get a list of stopwords in a specific language and filter the tokens using this list. Here is an example of how to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpFVqHskmIEh",
        "outputId": "061a6bbe-0ac4-4086-f636-6001274ab4e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens without stopwords: ['Natural', 'language', 'processing', 'field', 'artificial', 'intelligence', 'deals', 'interaction', 'computers', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# get list of stopwords in English\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "# remove stopwords\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "\n",
        "print(\"Tokens without stopwords:\", filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF-oc0xwmIEi"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "Reducing words to their root form by removing suffixes and prefixes, such as converting ‚Äújumping‚Äù to ‚Äújump.‚Äù  But it may produce non-existent words.\n",
        "\n",
        ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/stemming.png\n",
        "---\n",
        "name: 'stemming in nlp'\n",
        "width: 90%\n",
        "---\n",
        "Stemming in NLP\n",
        ":::\n",
        "\n",
        "To perform stemming on a list of tokens using NLTK, you can use the **nltk.stem.WordNetLemmatizer()** function to create a stemmer object and the method to stem each token. Here is an example of how to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhlI3RXQmIEi",
        "outputId": "ebf1d186-3238-40e4-83cc-f3665ac4a7d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemmed tokens: ['natur', 'languag', 'process', 'is', 'a', 'field', 'of', 'artifici', 'intellig', 'that', 'deal', 'with', 'the', 'interact', 'between', 'comput', 'and', 'human', '(', 'natur', ')', 'languag', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# create stemmer object\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# stem each token\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6EemtSmIEi"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "Reducing words to their base form by considering the context in which they are used, such as ‚Äúrunning‚Äù or ‚Äúran‚Äù becoming ‚Äúrun‚Äù. This technique is similar to stemming, but it is more accurate as it considers the context of the word.\n",
        "\n",
        ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/Lemmatization.png\n",
        "---\n",
        "name: 'lemmatization in nlp'\n",
        "width: 90%\n",
        "---\n",
        "Lemmatization in NLP\n",
        ":::\n",
        "\n",
        "To perform lemmatization on a list of tokens using NLTK, you can use the **nltk.stem.WordNetLemmatizer()** function to create a lemmatizer object and the **lemmatize()** method to lemmatize each token. Here is an example of how to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEH88DFSmIEi",
        "outputId": "cba51842-100c-4ac0-e288-d8e132b53d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatized tokens: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deal', 'with', 'the', 'interaction', 'between', 'computer', 'and', 'human', '(', 'natural', ')', 'language', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# create lemmatizer object\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# lemmatize each token\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "print(\"Lemmatized tokens:\", lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "brtPMI5mmIEj"
      },
      "source": [
        "###  Part Of Speech Tagging\n",
        "\n",
        "Identifying the part of speech of each word in the text, such as noun, verb, or adjective.\n",
        "\n",
        ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/POS tag and description.png\n",
        "---\n",
        "name: 'POS tag and description in nlp'\n",
        "width: 90%\n",
        "---\n",
        "POS tag and description\n",
        ":::\n",
        "\n",
        "To perform part of speech (POS) tagging on a list of tokens using NLTK, you can use the **nltk.pos_tag()** function to tag the tokens with their corresponding POS tags. Here is an example of how to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7cbYCngmIEj",
        "outputId": "9364e37c-5c45-492e-b06d-df883ddf6a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tagged tokens: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'IN'), ('deals', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('(', '('), ('natural', 'JJ'), (')', ')'), ('language', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# tag the tokens with their POS tags\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"Tagged tokens:\", tagged_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7zBSyeTmIEj"
      },
      "source": [
        "### Named Entity Recognition(NER)\n",
        "\n",
        "Extracting named entities from a text, like a person‚Äôs name.\n",
        "\n",
        ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/ner.gif\n",
        "---\n",
        "name: 'ner in nlp'\n",
        "width: 90%\n",
        "---\n",
        "ner in NLP\n",
        ":::\n",
        "\n",
        "To perform named entity recognition (NER) on a list of tokens using NLTK, you can use the **nltk.ne_chunk()** function to identify and label named entities in the tokens. Here is an example of how to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4YRFygYmIEj",
        "outputId": "47b13367-adac-4182-b438-8a45eed2d51d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Named entities: (S\n",
            "  Natural/JJ\n",
            "  language/NN\n",
            "  processing/NN\n",
            "  is/VBZ\n",
            "  a/DT\n",
            "  field/NN\n",
            "  of/IN\n",
            "  artificial/JJ\n",
            "  intelligence/NN\n",
            "  that/IN\n",
            "  deals/NNS\n",
            "  with/IN\n",
            "  the/DT\n",
            "  interaction/NN\n",
            "  between/IN\n",
            "  computers/NNS\n",
            "  and/CC\n",
            "  human/JJ\n",
            "  (/(\n",
            "  natural/JJ\n",
            "  )/)\n",
            "  language/NN\n",
            "  ./.\n",
            "  (PERSON John/NNP Smith/NNP)\n",
            "  works/VBZ\n",
            "  at/IN\n",
            "  (ORGANIZATION Google/NNP)\n",
            "  in/IN\n",
            "  (GPE New/NNP York/NNP)\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. John Smith works at Google in New York.\"\n",
        "\n",
        "# tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# tag the tokens with their part of speech\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "# identify named entities\n",
        "named_entities = nltk.ne_chunk(tagged_tokens)\n",
        "\n",
        "print(\"Named entities:\", named_entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRuH-KYLmIEj"
      },
      "source": [
        "## NLTK preprocessing pipeline example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi54buQYmIEk"
      },
      "source": [
        "Preprocessing techniques can be applied independently or in combination, depending on the specific requirements of the task at hand.\n",
        "\n",
        "Here is an example of a typical NLP pipeline using the NLTK:\n",
        "\n",
        "1.Tokenization: First, we need to split the input text into individual words (tokens). This can be done using the **nltk.word_tokenize()** function.\n",
        "\n",
        "2.Part-of-speech tagging: Next, we can use the **nltk.pos_tag()** function to assign a part-of-speech (POS) tag to each token, which indicates its role in a sentence (e.g., noun, verb, adjective).\n",
        "\n",
        "3.Named entity recognition: Using the **nltk.ne_chunk()** function, we can identify named entities (e.g., person, organization, location) in the text.\n",
        "\n",
        "4.Lemmatization: We can use the **nltk.WordNetLemmatizer()** function to convert each token to its base form (lemma), which helps with the analysis of the text.\n",
        "\n",
        "5.Stopword removal: We can use the **nltk.corpus.stopwords.words()** function to remove common words (stopwords) that do not add significant meaning to the text, such as ‚Äúthe,‚Äù ‚Äúa,‚Äù and ‚Äúan.‚Äù\n",
        "\n",
        "6.Text classification: Finally, we can use the processed text to train a classifier using machine learning algorithms to perform tasks such as sentiment analysis or spam detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRS95OzFmIEk"
      },
      "source": [
        "## NLTK preprocessing example code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIHQn3ozmIEk"
      },
      "source": [
        "First, we preprocess the text, including tokenization, part-of-speech tagging, named entity recognition, lemmatization and stopword removal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReWTh2romIEk"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# input text\n",
        "text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n",
        "\n",
        "# tokenization\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# part-of-speech tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# named entity recognition\n",
        "named_entities = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "# lemmatization\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# stopword removal\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "filtered_tokens = [token for token in tokens if token not in stopwords]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmp1kcBdmIEk"
      },
      "source": [
        "Then we do the text classification (example using a simple Naive Bayes classifier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHkCFBIxmIEk",
        "outputId": "77deb31c-2394-4da2-f6c7-8d1a131efe8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment: pos\n"
          ]
        }
      ],
      "source": [
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "# training data (using a toy dataset for illustration purposes)\n",
        "training_data = [(\"I enjoy the book.\", \"pos\"),(\"I like this movie.\", \"pos\"),(\"It was a boring movie.\", \"neg\")]\n",
        "\n",
        "# extract features from the training data\n",
        "def extract_features(text):\n",
        "    features = {}\n",
        "    for word in nltk.word_tokenize(text):\n",
        "        features[word] = True\n",
        "    return features\n",
        "\n",
        "# create a list of feature sets and labels\n",
        "feature_sets = [(extract_features(text), label) for (text, label) in training_data]\n",
        "# train the classifier\n",
        "classifier = NaiveBayesClassifier.train(feature_sets)\n",
        "\n",
        "# test the classifier on a new example\n",
        "test_text = \"I enjoyed the movie.\"\n",
        "print(\"Sentiment:\", classifier.classify(extract_features(test_text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0sYzYhGmIEk"
      },
      "source": [
        "## Your turn! üöÄ\n",
        "\n",
        "Assignment - [Beginner Guide to Text Pre-Processing](../../assignments/deep-learning/nlp/beginner-guide-to-text-preprocessing.ipynb)\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "Thanks to [Neri Van Otten](https://spotintelligence.com/author/spotintelligence) for creating the open-source project [Top 14 Steps To Build A Complete NLTK Preprocessing Pipeline In Python](https://spotintelligence.com/2022/12/21/nltk-preprocessing-pipeline).It inspire the majority of the content in this chapter."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}